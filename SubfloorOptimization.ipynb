{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StudiKasusTesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdjNy5/8c4A6q2CxlUk8kP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wahyudims/Subfloor-Structure-Optimization/blob/main/SubfloorOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzTcWSSA3799",
        "outputId": "8b3d98a4-255a-4c95-f3e5-a6224e1f3870"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUgxxkdvu5_"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I4f-vsj4eyN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrZ7FwnOwEaF"
      },
      "source": [
        "# Reading the dataset\n",
        "The dataset used is the dataset of various configuration of subfloor of commercial aircraft. The configuration parameters consist of Cross-section size, fibre orientation of composite, mass, and cross-section itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EuCLcql4UMd",
        "outputId": "168c17bf-6600-469f-daf8-c113bf26d8f4"
      },
      "source": [
        "df=pd.read_excel('/content/gdrive/MyDrive/Colab Notebooks/StudiKasus/DataBaru.xlsx')\n",
        "df=df.drop(df.iloc[:,0:1], axis=1)\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Ukuran_Cross_Section  Orientasi  ...  Force/mass         SEA\n",
            "0                    80         60  ...  173.264732  364.187705\n",
            "1                    88         59  ...  195.800441  380.579159\n",
            "2                    70         59  ...   93.012718  297.550636\n",
            "3                    72         59  ...  196.674517  602.044277\n",
            "4                    52         58  ...  167.928158  561.629402\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 8 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Ukuran_Cross_Section  150 non-null    int64  \n",
            " 1   Orientasi             150 non-null    int64  \n",
            " 2   Cross_Section         150 non-null    object \n",
            " 3   Mass                  150 non-null    float64\n",
            " 4   Force                 150 non-null    float64\n",
            " 5   EA                    150 non-null    float64\n",
            " 6   Force/mass            150 non-null    float64\n",
            " 7   SEA                   150 non-null    float64\n",
            "dtypes: float64(5), int64(2), object(1)\n",
            "memory usage: 9.5+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_mAyKr-wH_d"
      },
      "source": [
        "# Split the dataset into X and y variable \n",
        "from those configuration, there are several output produced which are the strength of subfloor structure (Force), Energy Absorption (EA), Strength per mass ratio (Force/mass), and Specific Energy Absorption (SEA). In this example we will focus on the output of Force/mass, so we will put the configuration parameters (Cross-section size, fibre orientation of composite, mass, and cross-section itself) into variable X and Force/mass into variable y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6H9aVPX_XQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72938a5d-49db-4430-b77b-deef66a4704d"
      },
      "source": [
        "X=df.iloc[:,0:4]\n",
        "y=df.iloc[:,6:7]\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhLtFP5eFyy-"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtS0VDT8wXPd"
      },
      "source": [
        "# Preprocessing Data\n",
        "The 'Cross_Section' column consist of three value which are ['Tophat', 'I-beam', 'C-beam']. These values are categorical value and need to be converted into numerical value so it can be processed by the algorithm. To convert it from categorical into numerical value, we need to do One Hot Encoding process which transforms the value into binary value (0 and 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FDO2e6LGQDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad8f042-25a5-43d3-a5ad-d036c49c94ac"
      },
      "source": [
        "ct = ColumnTransformer([(\"Cross_Section\", OneHotEncoder(), [2])], remainder = 'passthrough') # 2 is choosen because cross section has column index of 2\n",
        "X = np.array(ct.fit_transform(X), dtype=np.float64)\n",
        "print(X)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.        0.        1.       80.       60.        0.925347]\n",
            " [ 0.        0.        1.       88.       59.        1.06154 ]\n",
            " [ 0.        0.        1.       70.       59.        1.0615  ]\n",
            " [ 0.        0.        1.       72.       59.        1.0615  ]\n",
            " [ 0.        0.        1.       52.       58.        1.03449 ]\n",
            " [ 0.        0.        1.       72.       58.        0.886844]\n",
            " [ 0.        0.        1.       82.       57.        1.17211 ]\n",
            " [ 0.        0.        1.       54.       57.        1.02553 ]\n",
            " [ 0.        0.        1.       50.       57.        1.1721  ]\n",
            " [ 0.        0.        1.       40.       56.        0.575783]\n",
            " [ 0.        0.        1.       48.       56.        1.00751 ]\n",
            " [ 0.        0.        1.       58.       55.        1.131   ]\n",
            " [ 0.        0.        1.       74.       55.        0.848269]\n",
            " [ 0.        0.        1.       64.       55.        0.848269]\n",
            " [ 0.        0.        1.       56.       54.        0.693964]\n",
            " [ 0.        0.        1.       86.       54.        1.11043 ]\n",
            " [ 0.        0.        1.       82.       53.        0.544934]\n",
            " [ 0.        0.        1.       78.       53.        0.953593]\n",
            " [ 0.        0.        1.       44.       53.        0.544934]\n",
            " [ 0.        0.        1.       54.       52.        0.682253]\n",
            " [ 0.        0.        1.       66.       52.        0.682253]\n",
            " [ 0.        0.        1.       84.       51.        0.655434]\n",
            " [ 0.        0.        1.       62.       51.        0.917633]\n",
            " [ 0.        0.        1.       62.       51.        0.78656 ]\n",
            " [ 0.        0.        1.       42.       50.        0.89956 ]\n",
            " [ 0.        0.        1.       76.       50.        0.771161]\n",
            " [ 0.        0.        1.       48.       49.        0.755716]\n",
            " [ 0.        0.        1.       86.       49.        0.503801]\n",
            " [ 0.        0.        1.       44.       49.        0.503801]\n",
            " [ 0.        0.        1.       80.       48.        0.616849]\n",
            " [ 0.        0.        1.       58.       48.        0.740316]\n",
            " [ 0.        0.        1.       42.       47.        0.604026]\n",
            " [ 0.        0.        1.       60.       47.        0.604026]\n",
            " [ 0.        0.        1.       52.       47.        0.604026]\n",
            " [ 0.        0.        1.       46.       46.        0.827612]\n",
            " [ 0.        0.        1.       74.       46.        0.591158]\n",
            " [ 0.        0.        1.       88.       45.        0.7229  ]\n",
            " [ 0.        0.        1.       66.       45.        0.867523]\n",
            " [ 0.        0.        1.       46.       45.        1.0121  ]\n",
            " [ 0.        0.        1.       76.       44.        0.904796]\n",
            " [ 0.        0.        1.       60.       44.        0.565446]\n",
            " [ 0.        0.        1.       64.       43.        0.773697]\n",
            " [ 0.        0.        1.       70.       43.        0.663164]\n",
            " [ 0.        0.        1.       90.       43.        0.77369 ]\n",
            " [ 0.        0.        1.       68.       42.        0.647759]\n",
            " [ 0.        0.        1.       84.       42.        0.863673]\n",
            " [ 0.        0.        1.       50.       41.        0.730071]\n",
            " [ 0.        0.        1.       56.       41.        0.63232 ]\n",
            " [ 0.        0.        1.       68.       41.        0.421544]\n",
            " [ 0.        0.        1.       78.       40.        0.515043]\n",
            " [ 0.        1.        0.       50.       60.        0.46272 ]\n",
            " [ 0.        1.        0.       56.       59.        0.454969]\n",
            " [ 0.        1.        0.       58.       59.        0.682498]\n",
            " [ 0.        1.        0.       82.       59.        0.796073]\n",
            " [ 0.        1.        0.       70.       58.        0.447248]\n",
            " [ 0.        1.        0.       76.       58.        0.559091]\n",
            " [ 0.        1.        0.       84.       57.        0.769109]\n",
            " [ 0.        1.        0.       64.       57.        0.659306]\n",
            " [ 0.        1.        0.       40.       57.        0.549484]\n",
            " [ 0.        1.        0.       54.       56.        0.755609]\n",
            " [ 0.        1.        0.       46.       56.        0.755609]\n",
            " [ 0.        1.        0.       84.       55.        0.636105]\n",
            " [ 0.        1.        0.       68.       55.        0.742203]\n",
            " [ 0.        1.        0.       62.       55.        0.742203]\n",
            " [ 0.        1.        0.       88.       54.        0.83281 ]\n",
            " [ 0.        1.        0.       44.       54.        0.520506]\n",
            " [ 0.        1.        0.       48.       53.        0.715214]\n",
            " [ 0.        1.        0.       62.       53.        0.510867]\n",
            " [ 0.        1.        0.       86.       53.        0.715214]\n",
            " [ 0.        1.        0.       58.       52.        0.501228]\n",
            " [ 0.        1.        0.       44.       52.        0.801965]\n",
            " [ 0.        1.        0.       60.       51.        0.688225]\n",
            " [ 0.        1.        0.       78.       51.        0.688225]\n",
            " [ 0.        1.        0.       88.       51.        0.491589]\n",
            " [ 0.        1.        0.       66.       50.        0.38556 ]\n",
            " [ 0.        1.        0.       64.       50.        0.57834 ]\n",
            " [ 0.        1.        0.       54.       49.        0.472311]\n",
            " [ 0.        1.        0.       46.       49.        0.566773]\n",
            " [ 0.        1.        0.       72.       49.        0.755698]\n",
            " [ 0.        1.        0.       42.       48.        0.740275]\n",
            " [ 0.        1.        0.       52.       48.        0.740275]\n",
            " [ 0.        1.        0.       42.       47.        0.54364 ]\n",
            " [ 0.        1.        0.       80.       47.        0.54364 ]\n",
            " [ 0.        1.        0.       72.       47.        0.634246]\n",
            " [ 0.        1.        0.       76.       46.        0.354715]\n",
            " [ 0.        1.        0.       82.       46.        0.532073]\n",
            " [ 0.        1.        0.       80.       45.        0.607257]\n",
            " [ 0.        1.        0.       48.       45.        0.694008]\n",
            " [ 0.        1.        0.       78.       45.        0.433755]\n",
            " [ 0.        1.        0.       66.       44.        0.339293]\n",
            " [ 0.        1.        0.       60.       44.        0.424116]\n",
            " [ 0.        1.        0.       68.       43.        0.414477]\n",
            " [ 0.        1.        0.       70.       43.        0.497372]\n",
            " [ 0.        1.        0.       56.       43.        0.414477]\n",
            " [ 0.        1.        0.       74.       42.        0.485806]\n",
            " [ 0.        1.        0.       90.       42.        0.566773]\n",
            " [ 0.        1.        0.       74.       41.        0.395199]\n",
            " [ 0.        1.        0.       50.       41.        0.316159]\n",
            " [ 0.        1.        0.       86.       41.        0.474239]\n",
            " [ 0.        1.        0.       52.       40.        0.432672]\n",
            " [ 1.        0.        0.       56.       60.        0.578293]\n",
            " [ 1.        0.        0.       46.       59.        0.56876 ]\n",
            " [ 1.        0.        0.       60.       59.        0.454969]\n",
            " [ 1.        0.        0.       88.       59.        0.454969]\n",
            " [ 1.        0.        0.       66.       58.        0.559015]\n",
            " [ 1.        0.        0.       58.       58.        0.782687]\n",
            " [ 1.        0.        0.       60.       57.        0.879112]\n",
            " [ 1.        0.        0.       44.       57.        0.659379]\n",
            " [ 1.        0.        0.       78.       57.        0.659379]\n",
            " [ 1.        0.        0.       46.       56.        0.647787]\n",
            " [ 1.        0.        0.       48.       56.        0.539739]\n",
            " [ 1.        0.        0.       68.       55.        0.636105]\n",
            " [ 1.        0.        0.       78.       55.        0.530117]\n",
            " [ 1.        0.        0.       82.       55.        0.636105]\n",
            " [ 1.        0.        0.       86.       54.        0.728623]\n",
            " [ 1.        0.        0.       86.       54.        0.520463]\n",
            " [ 1.        0.        0.       66.       53.        0.5109  ]\n",
            " [ 1.        0.        0.       82.       53.        0.715298]\n",
            " [ 1.        0.        0.       70.       53.        0.817359]\n",
            " [ 1.        0.        0.       42.       52.        0.501187]\n",
            " [ 1.        0.        0.       88.       52.        0.501187]\n",
            " [ 1.        0.        0.       54.       51.        0.688302]\n",
            " [ 1.        0.        0.       56.       51.        0.393264]\n",
            " [ 1.        0.        0.       72.       51.        0.589838]\n",
            " [ 1.        0.        0.       90.       50.        0.578381]\n",
            " [ 1.        0.        0.       76.       50.        0.77112 ]\n",
            " [ 1.        0.        0.       76.       49.        0.566773]\n",
            " [ 1.        0.        0.       40.       49.        0.755729]\n",
            " [ 1.        0.        0.       84.       49.        0.472261]\n",
            " [ 1.        0.        0.       54.       48.        0.462635]\n",
            " [ 1.        0.        0.       64.       48.        0.370142]\n",
            " [ 1.        0.        0.       74.       47.        0.634326]\n",
            " [ 1.        0.        0.       42.       47.        0.362397]\n",
            " [ 1.        0.        0.       44.       47.        0.724794]\n",
            " [ 1.        0.        0.       64.       46.        0.709442]\n",
            " [ 1.        0.        0.       80.       46.        0.620675]\n",
            " [ 1.        0.        0.       62.       45.        0.607255]\n",
            " [ 1.        0.        0.       62.       45.        0.607255]\n",
            " [ 1.        0.        0.       68.       45.        0.607255]\n",
            " [ 1.        0.        0.       58.       44.        0.3393  ]\n",
            " [ 1.        0.        0.       80.       44.        0.59369 ]\n",
            " [ 1.        0.        0.       84.       43.        0.414459]\n",
            " [ 1.        0.        0.       72.       43.        0.414459]\n",
            " [ 1.        0.        0.       50.       43.        0.580335]\n",
            " [ 1.        0.        0.       50.       42.        0.485839]\n",
            " [ 1.        0.        0.       52.       42.        0.56671 ]\n",
            " [ 1.        0.        0.       74.       41.        0.474225]\n",
            " [ 1.        0.        0.       52.       41.        0.474225]\n",
            " [ 1.        0.        0.       48.       41.        0.553327]\n",
            " [ 1.        0.        0.       70.       40.        0.462703]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCkDo5Pqwf5F"
      },
      "source": [
        "As we can see that the 'Cross_Section' column transforms into the first three columns which consists of 1 and 0 value. [0 0 1] means it has Tophat cross-section, [0 1 0] means it has I-beam cross-section, and [0 0 1] means it has C-beam cross-section. After that we need to split the data into training data and test data. For building a machine learning model, we need to train our model on the training set. And for checking the performance of our model, we use a Test set. There are 4 variables which are X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol7Wk1rrJWkl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) # 0.2 means 20% of the data will be used as test data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzFeopOgwqIV"
      },
      "source": [
        "Scale the input data to make sure all feature has the same range of value so no feature could heavily dominates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FRP9GR_BQf"
      },
      "source": [
        "sc = MinMaxScaler() # Scaler that is used in this case is MinMaxScaler\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "y_train = sc.fit_transform(y_train)\n",
        "y_test = sc.transform(y_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkj8L61BP_ll"
      },
      "source": [
        "# Processing data: making Artificial Neural Network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLm_b9GcxQ69"
      },
      "source": [
        "Build Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj4zV1V_AR1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c80a14-1ecd-48e2-d621-40db151fe68e"
      },
      "source": [
        "regressor = Sequential()#Sequential class needed to build ANN as sequence of layers\n",
        "\n",
        "#Add input layer and first hidden layer to ANN Architecture\n",
        "regressor.add(Dense(6, activation = 'linear', input_dim = 6))\n",
        "\n",
        "#Add another layers in ANN Architecture\n",
        "regressor.add(Dense(6, activation = 'linear'))\n",
        "regressor.add(Dense(6, activation = 'linear'))\n",
        "regressor.add(Dense(6, activation = 'linear'))\n",
        "\n",
        "#Add output layer in ANN Arcitecture\n",
        "regressor.add(Dense(1, activation = 'linear'))\n",
        "#Compiling ANN Using Sthocastic Gradient Descent\n",
        "\n",
        "#Compile the model\n",
        "regressor.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics = ['mse'] )\n",
        "\n",
        "\n",
        "\n",
        "#3. Fitting ANN to the Optimization Regression Model\n",
        "#Fitting to training set\n",
        "history = regressor.fit(X_train, y_train, batch_size = 10, epochs = 300, validation_data=(X_test,y_test))\n",
        "y_predict = regressor.predict(X_test)\n",
        "print(history.history.keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "12/12 [==============================] - 1s 16ms/step - loss: 0.2695 - mse: 0.2695 - val_loss: 0.1181 - val_mse: 0.1181\n",
            "Epoch 2/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.0395 - val_mse: 0.0395\n",
            "Epoch 3/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0192 - val_mse: 0.0192\n",
            "Epoch 4/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0177 - val_mse: 0.0177\n",
            "Epoch 5/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0156 - val_mse: 0.0156\n",
            "Epoch 6/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0124 - val_mse: 0.0124\n",
            "Epoch 7/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0124 - val_mse: 0.0124\n",
            "Epoch 8/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0125 - val_mse: 0.0125\n",
            "Epoch 9/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 10/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 11/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 12/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 13/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0105 - val_mse: 0.0105\n",
            "Epoch 14/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 15/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0107 - val_mse: 0.0107\n",
            "Epoch 16/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 17/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 18/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0107 - val_mse: 0.0107\n",
            "Epoch 19/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0108 - val_mse: 0.0108\n",
            "Epoch 20/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 21/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0108 - val_mse: 0.0108\n",
            "Epoch 22/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 23/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 24/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 25/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 26/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 27/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 28/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 29/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 30/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0108 - val_mse: 0.0108\n",
            "Epoch 31/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 32/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 33/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 34/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 35/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 36/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 37/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 38/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 39/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 40/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 41/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 42/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 43/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 44/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 45/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 46/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 47/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 48/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 49/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 50/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 51/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 52/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 53/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 54/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 55/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 56/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 57/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 58/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 59/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 60/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 61/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 62/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 63/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 64/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 65/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 66/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0107 - val_mse: 0.0107\n",
            "Epoch 67/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 68/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 69/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 70/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 71/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 72/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 73/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 74/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 75/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 76/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0125 - val_mse: 0.0125\n",
            "Epoch 77/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 78/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 79/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 80/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 81/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 82/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 83/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 84/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 85/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 86/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 87/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 88/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 89/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 90/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 91/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 92/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 93/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 94/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 95/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 96/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 97/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 98/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 99/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 100/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 101/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 102/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 103/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0126 - val_mse: 0.0126\n",
            "Epoch 104/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 105/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 106/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 107/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 108/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 109/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 110/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 111/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 112/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 113/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 114/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 115/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 116/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 117/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 118/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 119/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 120/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 121/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 122/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 123/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 124/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 125/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 126/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 127/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 128/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 129/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 130/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 131/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 132/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 133/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 134/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 135/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 136/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 137/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 138/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 139/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0125 - val_mse: 0.0125\n",
            "Epoch 140/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 141/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 142/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 143/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 144/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 145/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0126 - val_mse: 0.0126\n",
            "Epoch 146/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 147/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 148/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 149/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 150/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 151/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 152/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 153/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 154/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 155/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 156/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 157/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 158/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 159/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 160/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 161/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 162/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0108 - val_mse: 0.0108\n",
            "Epoch 163/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 164/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 165/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 166/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 167/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 168/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 169/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 170/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 171/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 172/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 173/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 174/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 175/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 176/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 177/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 178/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 179/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 180/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 181/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0123 - val_mse: 0.0123\n",
            "Epoch 182/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 183/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 184/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 185/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 186/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 187/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 188/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 189/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 190/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 191/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 192/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 193/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 194/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 195/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 196/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 197/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 198/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 199/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 200/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 201/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 202/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 203/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 204/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 205/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 206/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 207/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 208/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 209/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 210/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 211/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 212/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 213/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 214/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 215/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 216/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 217/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 218/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 219/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 220/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 221/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 222/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 223/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 224/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 225/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 226/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 227/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 228/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 229/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 230/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 231/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 232/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 233/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 234/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 235/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 236/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 237/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 238/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 239/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 240/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 241/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 242/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0125 - val_mse: 0.0125\n",
            "Epoch 243/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 244/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0133 - val_mse: 0.0133\n",
            "Epoch 245/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 246/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 247/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0110 - val_mse: 0.0110\n",
            "Epoch 248/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 249/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 250/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 251/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 252/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 253/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 254/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 255/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 256/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 257/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 258/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 259/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 260/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 261/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 262/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 263/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 264/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 265/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 266/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 267/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 268/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0120 - val_mse: 0.0120\n",
            "Epoch 269/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 270/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 271/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 272/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0121 - val_mse: 0.0121\n",
            "Epoch 273/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 274/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 275/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 276/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 277/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 278/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 279/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 280/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 281/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 282/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 283/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0118 - val_mse: 0.0118\n",
            "Epoch 284/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0113 - val_mse: 0.0113\n",
            "Epoch 285/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 286/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "Epoch 287/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 288/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 289/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 290/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 291/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0122 - val_mse: 0.0122\n",
            "Epoch 292/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 293/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 294/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0117 - val_mse: 0.0117\n",
            "Epoch 295/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0112 - val_mse: 0.0112\n",
            "Epoch 296/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0119 - val_mse: 0.0119\n",
            "Epoch 297/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0116 - val_mse: 0.0116\n",
            "Epoch 298/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 299/300\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0115 - val_mse: 0.0115\n",
            "Epoch 300/300\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0111 - val_mse: 0.0111\n",
            "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n4ZCk-excQt"
      },
      "source": [
        "Plotting the train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkYrnCbvBHHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "673634e9-d385-4678-dd76-902e8e5fbc44"
      },
      "source": [
        "#Visualization Data\n",
        "#Plot training & validation accuracy\n",
        "plt.plot(history.history['loss'], 'magenta')\n",
        "plt.plot(history.history['val_loss'], 'g--')\n",
        "plt.title('Model Loss Strength/Weight')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwBflSGPxukl"
      },
      "source": [
        "As we can see from the plot of train data and test data, the train and test plot seem close to each other. As we can see after 300 iterations the loss value is 0.0120 and val_loss is 0.0113 which are quite small. It means the results is quite accurate. Next we will extract weight and bias from ANN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yROLms1GWOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40751e0-1abd-4ed2-920a-0ec454932a2d"
      },
      "source": [
        "#Model Evaluation\n",
        "model = regressor.get_weights()\n",
        "DFmodel = pd.DataFrame(model)\n",
        "\n",
        "#Regression Model from ANN\n",
        "n = len(model)\n",
        "bias = []\n",
        "weight = []\n",
        "for i in range (0,n):\n",
        "        if i%2 == 1:\n",
        "            bias.append(model[i])\n",
        "        else:\n",
        "            weight.append(model[i])\n",
        "# print(bias)\n",
        "# print(weight)\n",
        "#Regression Bias\n",
        "a = []\n",
        "for i in range (0,len(bias)):\n",
        "    if i < (len(bias)-1):\n",
        "        w = np.identity(y.shape[1])\n",
        "        for j in range (0, (len(bias) - i-1)):\n",
        "            r = np.array(weight[len(bias)-j-1])\n",
        "            w = np.matmul(w,np.transpose(r))\n",
        "        bn = np.matmul(w,bias[i])\n",
        "        a.append(bn)\n",
        "    else:\n",
        "        bn = bias[i]\n",
        "        a.append(bn)\n",
        "\n",
        "a_reg = sum(a)\n",
        "\n",
        "#Regression Weight\n",
        "b_reg = np.identity(y.shape[1])\n",
        "for j in range (0, (len(bias))):\n",
        "    r = np.array(weight[len(bias)-j-1])\n",
        "    b_reg = np.matmul(b_reg,np.transpose(r))\n",
        "\n",
        "FPW = [a_reg, b_reg]\n",
        "print(FPW)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([-0.32050398]), array([[ 0.29183182,  0.81402033,  0.63242631,  0.23803562, -0.11705439,\n",
            "         0.37862717]])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  values = np.array([convert(v) for v in values])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQdxvzCQNJz"
      },
      "source": [
        "As we can see there are 2 arrays. The first array represents bias value which are -0.32050398. The second value represents weight value for each configuration parameters which are in order C-beam, I-beam, Tophat, Cross-section size, Fibre orientation of composite, and mass. The weight value represents the effect of its parameters into the output in this case is the Force/mass value. The bigger the weight value means the configuration parameter will affect the output more compared to other parameters. From the weight values we can conclude that the paramater that has the most impact to the output are I-beam > Tophat > mass > C-beam > Cross-section size > and Fibre orientation of composite. From this conclusion we know which parameter need to be focused more in contributing to the output. This finding can be used as base knowledge to optimize subfloor structure of an aircraft. "
      ]
    }
  ]
}